<h1>Johanna Nguyen, Tyler Zamski</h1>
<hr>
<h2>Task 1</h2>
Originally, we chose to show the data distribution between each of the three classes using Histograms. 
<div>
  <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/distribution1.png?raw=true" height="300" width="350">
  <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/distribution2.png?raw=true" height="300" width="350">
  <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/distribution3.png?raw=true" height="300" width="350">
</div>
Later we realized that having three separate histograms was a poor visualization of the data imbalance so we created a bar graph with each class along with their corresponding frequencies. Since there was a data imbalance with most of the data maintaining a <b>fetal_health</b> classification of <b>1</b>, we use a Stratified Train-Test Split which splits the data to preserve the same proportions of data in each class.
<div>
  <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/distribution4.png?raw=true" height="300" width="350">
</div>
<hr>

<h2>Task 2</h2>
To find the 10 most reflective features of fetal health, we calculated the Pearon's Correlation Coefficient of each feature using the <b>scipy.stats.pearsonr</b> function. Once that was calculated, we sorted their absolute values in ascending order and extracted the last 10 from the list. Since <b>scipy.stats.pearsonr</b> returns both the Correlation Coefficient and a pvalue, we compared the pvalue against <b>0.1</b> and <b>0.05</b> to prove that the correlation is statistically significant based on 90% and 95% confidence intervals.<br><br>

<b>uterine_contractions</b><br>
Correlation: 0.20489372127986774<br>
P-value: 1.3846747025644431e-21<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>histogram_median</b><br>
Correlation: 0.20503299554125773<br>
P-value: 1.2988891337541575e-21<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>histogram_variance</b><br>
Correlation: 0.20662962204272547<br>
P-value: 6.218926688568785e-22<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>mean_value_of_long_term_variability</b><br>
Correlation: 0.22679706542348083<br>
P-value: 3.33667040120377e-26<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>histogram_mean</b><br>
Correlation: 0.22698517650772904<br>
P-value: 3.030067468415955e-26<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>histogram_mode</b><br>
Correlation: 0.2504118122228778<br>
P-value: 9.305867662128216e-32<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>accelerations</b><br>
Correlation: 0.36406579288786367<br>
P-value: 1.2435259928479686e-67<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>percentage_of_time_with_abnormal_long_term_variability</b><br>
Correlation: 0.42614641992406527<br>
P-value: 1.5027669343305033e-94<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>abnormal_short_term_variability</b><br>
Correlation: 0.47119075284667544<br>
P-value: 5.9217105098313305e-118<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>

<b>prolongued_decelerations</b><br>
Correlation: 0.48485918632134833<br>
P-value: 8.854416080655132e-126<br>
Significance Test 90%: True<br>
Significane Test 95%: True<br><br>
<hr>

<h2>Task 3</h2>
For our two models we chose a decision tree and a gaussian naive bayes. We selected these two classifier models as they both highly accurate with <b>multiclass problems</b> and typically produce comparable results, with the primary difference between them being that GNB assumes classes are independent of each other and the decision tree does not. We used the recommended 70/30 Train-Test Split. For consistency’s sake, we also used the same random state, so that the results of our models would be reproducible and comparable. Below is the visualization of our decision tree model.
<div>
    <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/decisiontree.png?raw=true" height="300" width="350">
</div>
<hr>

<h2>Task 4</h2>
We used the <b>heatmap</b> function from the <b>seaborn module</b> to create a 3x3 visualization of our confusion matrix. Each matrix, seen below, highlights the data imbalance of the dataset, as you can see the majority of predicted values are being correctly classified as <b>Normal</b>. Though the data could have been binarized into a 2x2 matrix, the 3x3 matrix still provides all relevant information, specifically the calculations for true/false positive/negative values for each class. Two of the most relevant of these calculations, TPR and FNR, are as follows.<br><br>

<b>True Positive Ratios:</b><br>
Decision Tree = 567/638 = 0.88871 ~= <b>89%</b><br>
GNB = 511/638 = 0.80094 ~= <b>80%</b><br><br>

<b>False Negative Ratios:</b><br>
Decision Tree = 63/638 = 0.09874 ~= <b>10%</b><br>
GNB = 126/638 = 0.19749 ~= <b>20%</b><br><br>
<div>
    <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/confusionmatrix1.png?raw=true" height="300" width="350">
    <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/confusionmatrix2.png?raw=true" height="300" width="350">
</div>
<hr>

<h2>Task 5</h2>
We used the <b>roc_auc_score</b> and  <b>f1_score</b> functions from the <b>sklearn.metrics</b> module to calculate the area under the ROC curve and the F1 scores, respectively. Since this is a multiclass problem, certain parameters need to be modified. For the ROC AUC function, we set the multiclass parameter to <b>ovo</b> for one-vs-one, and for the F1 score we modified the average parameter. Initially, we used the <b>macro</b> option, however this doesn’t account for label imbalance, which is an issue with our dataset, so we later changed it to <b>micro</b> (this change is not present in early documentation, such as the powerpoint, and thus the F1 score are lower in those prior cases). Finally, there wasn’t a clean way to calculate the Precision-Recall area under the curve, as this was again a multiclass problem, however we were able to approximate this value with the <b>average_precision_score</b> function. In order to use this function, we first had to binarize our fdy_test and y_pred variables, using the <b>label_binarize</b> function from <b>sklearn.preprocessing</b>. Provided below are the results from our scorings for each model.<br><br>

<b>DECISION TREE SCORES</b><br>
ROC AUC Score:           0.8638349896405305<br>
F1 Score:                0.8887147335423197<br>
Average Precision Score: 0.8324974346871644<br><br>

<b>GNB SCORES</b><br>
ROC AUC Score:           0.8859486248537537<br>
F1 Score:                0.8009404388714735<br>
Average Precision Score: 0.8170993441891052<br><br>
<hr>

<h2>Task 6</h2>
For K-Means clustering, we used the <b>KMeans</b> function from <b>sklearn.cluster</b>. This was used to create the different clusters once the data was plotted. We decided to plot <b>baseline value</b> and <b>fetal_movement</b> on a scatterplot. As shown through the figures, the fetuses had a higher movement when their baseline values (in beats per minute) were closer to 120. If their heart rate fell lower or higher than that, that signaled an implication and therefore they had less movements.
<div>
  <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/kmeans1.png?raw=true" height="300" width="350">
  <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/kmeans2.png?raw=true" height="300" width="350">
  <img src="https://github.com/johannanguyen/fetal_health/blob/master/screenshots/kmeans3.png?raw=true" height="300" width="350">
</div>
<hr>
